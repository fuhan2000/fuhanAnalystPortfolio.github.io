<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Text Processing for NLP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class=""></a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Portfolio Main Page</a></li>

						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/fuhhanchang/" class="icon brands alt fa-linkedin"><span class="label">linkedin</span></a></li>
							<li><a href="https://github.com/fuhan2000" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									
									<h1>Text Processing for NLP<br />
										</h1>
									<p>Here we cleaned up the text and use spacy en_core_web_md to do  <br/>
										lemmatization.
									</p>
								</header>

								<div class="image main"><img src="images/nlp_scikit_spacy/nlp_scikit_spacy.png" alt="" /></div>
								<p>I will clean the data, removed accented text, use spacy lemmatizer, remove special characters, remove multiple spaces, remove some popular and rare 
									words. This text cleaning is more sophisticated than my earlier <a href="scikit_NLP_CountVec.html">CountVectorizer</a> and <a href="scikit_NLP_tfid.html">TfidfVectorizer</a>. 
									The text cleaning is tested on a dummy dataframe before I import actual amazon data. I create a balanced training dataset which can then be fed into a 
									CountVectorizer or TfidfVectorizer. </p>
									<p>I conclude by discussing why I did not use spacy vector to train a model on amazon dataset and where it is ok to use 
									spacy vector for training.
								</p>

								<h2>Key functions</h2>
								<p>I wrote:</p>
								<ul>
									<li>clean_text() to do some text cleaning with help from re and contractions library</li>
									<li>remove_accented() to remove text like "áccéñtéd téxt"</li>
									<li>lemmatize_df() to reduce words to its base form</li>
									<li>rm_special_df() to handle special characters</li>
									<li>rm_multiple_df() to handle multiple spaces in a string</li>
									<li>functions to remove popular and rare words</li>
									
								</ul>
								<h2>clean_text()</h2>  
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/clean_text.png" alt=""></div></figure>
								<p>This function will remove http addresses, emails, US phone numbers, US dates and html tags. It will then do a contraction to expansion of certain text like won't to will not. </p>
								<div class="image main"><img src="images/nlp_scikit_spacy/import.png" alt="" /></div>
								<p>I created a dummy df to make sure my function can work. </p>

								<h2>remove_accented()</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/remove.png" alt=""></div></figure>
								<p>This help to remove accented text which may be used by hispanics in amazon reviews.</p>

								<h2>lemmatize_df()</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/lemma.png" alt=""></div></figure>
								<p>This help to reduce word to its base form. There are other choices beside spacy. More information <a href="https://www.machinelearningplus.com/nlp/lemmatization-examples-python/">here</a>.</p>

								<h2>rm_special_df()</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/special.png" alt=""></div></figure>
								<p>This help to remove all the special characters.</p>

								<h2>rm_multiple_df()</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/rm_multiple.png" alt=""></div></figure>
								<p>This help to remove multiple spaces in a string. </p>

								<H2>Import amazon data into dataframe</H2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/json.png" alt=""></div></figure>
								<p>More information on the amazon data can be found at Jianmo Ni of UCSD <a href="https://nijianmo.github.io/amazon/index.html">here</a>.</p>

								<h2>text cleaning</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/clean_df.png" alt=""></div></figure>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/rm_spe.png" alt=""></div></figure>
								<p>We apply all these functions to our dataframe.</p>

								<h2>Too many 5 stars review</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/too_many_5.png" alt=""></div></figure>
								<p>We have too many five stars reviews. So I will just encode a five as one and the rest as zero.</p>

								<h2>Frequency of words</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/freq_of.png" alt=""></div></figure>
								<p>text is a very long string containing all the contents of df['reviewText']. mylist split up the text into a list. 
									df_2 is a pandas series and we make use of the value_counts() to give us the frequency of words.</p>
								
								<h2>Frequent words</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/top25.png" alt=""></div></figure>
								<p>I decided to exclude 'not' from the list. 'Not interesting to read' may be important to my model.</p>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/top24.png" alt=""></div></figure>
								<p>So i have 24 words that I considered popular.</p>
								
								<h2>Rare words</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/rare.png" alt=""></div></figure>
								<p>These are the 25 rare words.</p>

								<h2>Remove top24 frequent words</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/rem_my.png" alt=""></div></figure>
								<p>We remove the top 24 frequent words. </p>

								<h2>Remove least frequent 25 words</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/rm_bott.png" alt=""></div></figure>
								<p>Here we remove the least popular words. </p>

								<h2>Train test split</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/training.png" alt=""></div></figure>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/test_x.png" alt=""></div></figure>

								<h2>Balanced training set</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/training1.png" alt=""></div></figure>
								<p>training_1 and training_0 will have 2260 rows each. </p>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/training_01.png" alt=""></div></figure>
								<p>training_01 will now have a balanced training set with 2260 ones and 2260 zeroes. The ones and zeroes labels have been randomly distributed.</p>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/50_50.png" alt=""></div></figure>

								<h2>Model training</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/fit_trans.png" alt=""></div></figure>
								<p>We can then use train_x, train_y to train our model. In the above picture, you can use TfidfVectorizer. </p>

								<h2>Why I did not use spacy vector to train a model</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/actual_txt.png" alt=""></div></figure>
								<p>The above text is from the cleaned amazon dataset and is quite long. The text vector has 300 dimensions. This vector is made up of the average of the word 
									vectors over each word in the sentence. </p>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/text2.png" alt=""></div></figure>
								<p>text2 is a short text and has the same dimensions. Because the average is made over fewer words, this vector will be better at capturing the meaning of the 
									sentence. ie., the vector is better at capturing what the reviewer is trying to convey. </p>
								<p>So if I train my model on spacy vectors over long text, it might give worse results than bag of words model.</p>

								<h2>Where I can use spacy vector</h2>
								<figure style="max-width:2388pxpx" class="w-richtext-align-fullwidth w-richtext-figure-type-image"><div><img src="images/nlp_scikit_spacy/nlp_scikit_spacy.png" alt=""></div></figure>
								<p>But suppose the reviews are short like these in train_list, then we can get good prediction. Notice this model is able to predict correctly for not interesting, 
									poorly written, lousy author and good writer even though these words do not appear in train_list. So spacy is good if the reviews are short and to the point. </p>
								
								<h2>Conclusion</h2>
								<p>Spacy is good for short text. Because there is so much power baked into spacy en_core_web_md, with just six training examples, it is able to make four correct predictions.
									This shows how important text cleaning is. Just be aware of spacy shortcomings for long text.
								</p>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<nav id="nav">
							<ul class="links">
								<li><a href="index.html">Portfolio Main Page</a></li>
	
							</ul>
						</nav>

						<section class="split contact">




						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; fuhan</li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>